# Fine-tuning 微调

了解如何为应用程序自定义模型。

## 介绍 

::: tip 提示

本指南适用于新的OpenAI微调API的用户。如果您是旧版的微调用户，请参考我们的[旧版微调指南](../embeddings/embeddingsPage.md)。

:::

通过提供微调功能，您可以更充分地利用API提供的模型，它可以提供以下优势：

- 更高质量的结果：微调可以提供比仅使用提示更高质量的输出结果。通过微调，模型可以根据特定任务或领域的训练数据进行优化，从而生成更准确、更相关的结果。

- 可以训练更多示例：微调允许您使用更多的示例进行训练，而这些示例无法完全包含在一个提示中。通过提供更多的训练数据，微调可以提高模型的泛化能力和性能。

- 减少令牌使用量：由于提示的缩短，微调可以节省令牌使用量。较短的提示意味着模型需要处理的令牌数量更少，从而减少了整体计算和处理的负担。

- 降低延迟请求：由于微调模型已经在特定任务上进行了训练和优化，因此可以提供更低延迟的请求响应。这意味着您可以更快地获取模型生成的结果，从而提高交互性和实时性。

OpenAI的文本生成模型已经在大量文本上进行了预训练。为了有效地使用这些模型，我们在提示中包含了指令和有时甚至多个示例。使用示例来展示如何执行任务通常被称为“少样本学习”。

微调通过在许多示例上进行训练，提供了比提示中能容纳的更多的示例，从而改进了少样本学习的效果，使您能够在许多任务上获得更好的结果。一旦模型经过微调，您就不需要在提示中提供那么多的示例。这样可以节省成本，并实现更低延迟的请求。

总而言之，微调通过在更多示例上进行训练，提供了比少样本学习更好的性能，使您能够在广泛的任务上获得更好的结果。一旦模型经过微调，您就可以减少在提示中提供示例的数量，从而节省成本并实现更低延迟的请求。

从高层次上看，微调包括以下步骤：

- 1.准备和上传训练数据
- 2.训练新的微调模型
- 3.评估结果，如果需要可以返回到步骤1
- 4.使用您的微调模型

访问我们的[定价页面](https://openai.com/api/pricing)，了解有关微调模型训练和使用的计费方式的更多信息。

## 哪些模型可以微调？

::: tip 提示
GPT-4的微调是通过实验性访问计划进行的，符合条件的用户可以在微调界面中申请访问权限。
:::

目前可以对以下模型进行微调：

- 1.gpt-3.5-turbo-1106（推荐）
- 2.gpt-3.5-turbo-0613
- 3.babbage-002
- 4.davinci-002
- 5.gpt-4-0613（实验性 - 符合条件的用户将在[微调界面](https://platform.openai.com/finetune)中看到请求访问权限的选项），在创建新的微调任务时可用。

您还可以对已经进行过微调的模型进行再微调，这在您获取了额外的数据并且不想重复之前的训练步骤时非常有用。

我们预计gpt-3.5-turbo模型在结果和易用性方面对大多数用户来说都是合适的选择，除非您是从一个传统的微调模型迁移过来的。

## 何时使用微调？

微调OpenAI的文本生成模型可以使它们在特定应用中表现更好，但这需要仔细投入时间和精力。我们建议首先尝试通过提示工程、提示链（将复杂任务分解为多个提示）和函数调用来获得良好的结果，主要原因如下：

在许多任务中，我们的模型可能一开始看起来表现不佳，但通过正确的提示，结果可以得到改善，因此可能不需要进行微调。

与需要创建数据集和运行训练作业的微调相比，迭代提示和其他策略具有更快的反馈循环。

在仍然需要微调的情况下，最初的提示工程工作并不是浪费的 - 我们通常在微调数据中使用良好的提示（或将提示链/工具使用与微调相结合）时会得到最佳结果。

总之，通过正确的提示工程技巧，可以改进模型在许多任务中的表现，而不需要进行微调。提示工程的迭代过程比微调具有更快的反馈循环，并且在需要微调时，良好的提示工程工作可以为微调的结果提供更好的起点。

我们的提示工程[指南](../embeddings/embeddingsPage.md)提供了一些最有效的策略和技巧的背景，可以帮助您在不进行微调的情况下获得更好的性能。您可能会发现在我们的[playground](https://platform.openai.com/playground)中快速迭代提示非常有帮助。


## 常见用例

一些常见的使用案例，细调可以改善结果：

- 设置风格、语气、格式或其他定性因素
- 提高产生所需输出的可靠性
- 纠正不能按照复杂提示进行操作的错误
- 以特定方式处理许多边缘情况
- 执行难以用提示清楚表达的新技能或任务


一个高层次的思考这些案例的方法是更容易“展示，而不是告诉”。在接下来的部分中，我们将探讨如何为微调设置数据以及各种示例，其中微调可以提高基线模型的性能。

另一个微调有效的场景是通过替换GPT-4或利用较短的提示来降低成本和/或延迟，而不会牺牲质量。如果您可以通过GPT-4获得良好的结果，通常可以通过在GPT-4完成的基础上进行微调，可能还可以使用缩短的指令提示，在微调gpt-3.5-turbo模型上达到类似的质量。

## 准备数据集

一旦确定微调是正确的解决方案（即您已经优化了提示，但模型仍存在问题），您需要准备数据来训练模型。您应该创建一组多样化的示范对话，这些对话与您在生产环境中要求模型回答的对话类似。

数据集中的每个示例都应该是一个对话，格式与我们的Chat Completions API相同，具体来说，是一个消息列表，每个消息都有一个角色、内容和可选的名称。至少一些训练示例应直接针对提示模型不符合预期的情况，提供的助手消息应该是您希望模型提供的理想回应。

## 示例格式

在这个例子中，我们的目标是创建一个偶尔给出讽刺回应的聊天机器人，以下是我们可以为数据集创建的三个训练示例（对话）：

```json 
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?"}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}
```

微调gpt-3.5-turbo需要使用对话式聊天格式。对于babbage-002和davinci-002，您可以按照下面展示的[旧版微调](../embeddings/embeddingsPage.md)所使用的提示完成对格式进行操作。

```json
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
```
## 构建提示

我们通常建议在微调之前，将您发现的最佳模型指令和提示集合，包含在每个训练样例中。这样做可以让您获得最好且最通用的结果，特别是当您只有相对较少的训练样例时（例如不到一百个）。

如果您想要缩短在每个样例中重复的指令或提示，以节省成本，请记住模型可能会表现得好像这些指令已包含在内，并且在推理时很难让模型忽略这些“内建”的指令。

为了得到良好的结果，可能需要更多的训练样例，因为模型必须完全通过示范学习，而没有引导性的指导说明。

## 关于示例数量的建议

要对模型进行微调，您需要提供至少10个示例。通常情况下，使用gpt-3.5-turbo模型进行微调时，我们会看到在50到100个训练示例上有明显的改进，但实际使用情况会有很大的变化。

我们建议从精心设计的50个示范开始微调，并观察模型是否在微调后显示出改进迹象。在某些情况下，这可能足够了，但即使模型还不完全达到生产质量，明显的改进也是一个好兆头，这意味着提供更多的数据将继续改进模型。如果没有改进，这可能表明您需要重新考虑如何为模型设置任务或重组数据，以便在超越有限示例集之前进行扩展。

## 训练集和测试集的划分

在收集初始数据集之后，我们建议将其分割为训练和测试部分。在提交微调任务时，我们将提供有关训练和测试文件的统计信息，以了解模型的改进程度。此外，在早期构建一个测试集对于确保在训练之后能够评估模型非常有用，可以通过在测试集上生成样本来进行评估

## Token 限制

Token限制取决于您选择的模型。对于gpt-3.5-turbo-1106模型，最大上下文长度为16,385个Token，因此每个训练示例也限制为16,385个Token。对于gpt-3.5-turbo-0613模型，每个训练示例限制为4,096个Token。超过默认长度的示例将被截断为最大上下文长度，从训练示例的末尾删除Token。为确保整个训练示例适合上下文，请检查消息内容中的总Token数是否在限制范围内。

您可以使用我们在 OpenAI cookbook 提供的 [counting tokens notebook](https://cookbook.openai.com/examples/How_to_count_tokens_with_tiktoken.ipynb)来计算Token数量。


## 估计成本

请参考[pricing page](https://openai.com/pricing)，了解每1千个输入和输出tokens的费用（我们不收取验证数据中的tokens费用）。要估算特定微调作业的费用，使用以下公式：

```js 
每1千个标记的基本费用 * 输入文件中的token数量 * 训练的时期数量
```
对于一个包含10万个token的训练文件，训练3个时期，预计费用约为2.40美元。

## 检查数据格式

在编译数据集并创建微调作业之前，检查数据格式非常重要。为此，我们创建了一个简单的Python脚本，您可以使用它来查找潜在的错误、查看标记计数并估算微调作业的费用。

<checkDataFormat/>


## 上传训练文件

验证数据后，需要使用文件API上传文件，以便与微调作业一起使用：

<uploadFile/>

在上传文件后，需要一些时间进行处理。在文件处理过程中，您仍然可以创建微调作业，但作业将在文件处理完成后才开始运行。

最大文件上传大小为1GB，但我们不建议使用那么多数据进行微调，因为您可能不需要那么大量的数据来看到改进。


## 创建一个 fine-tuning 模型 

确保您的数据集具有正确的数量和结构，并已上传文件后，下一步是创建一个微调任务。我们支持通过微调UI或以编程方式创建微调任务。

使用OpenAI SDK启动微调工作的步骤如下：

<creatModelOne/>

在这个例子中，model是你想要微调的模型的名称（gpt-3.5-turbo、babbage-002、davinci-002或已存在的微调模型），training_file是当训练文件上传到OpenAI API时返回的文件ID。你可以使用[suffix parameter](../capabilityComponents/textGeneration.md)自定义微调模型的名称。

要设置额外的微调参数，如验证文件（validation_file）或超参数（hyperparameters），请参考[微调的API规范](../embeddings/embeddingsPage.md)。

在您启动了微调作业之后，可能需要一些时间才能完成。您的作业可能会排在我们系统中其他作业的后面，训练模型的时间取决于模型和数据集的大小，可能需要几分钟或几小时。在模型训练完成后，创建微调作业的用户将收到电子邮件确认。

除了创建微调作业外，您还可以列出现有的作业，获取作业的状态或取消作业。

<creatModelTwo/>

## 使用微调模型


当作业成功时，当您检索作业详情时，您将在fine_tuned_model字段中看到填充有模型的名称。现在，您可以将此模型指定为[Chat Completions](../fineTuning/fineTuning.md)（用于gpt-3.5-turbo）或[legacy Completions API](../embeddings/embeddingsPage.md)（用于babbage-002和davinci-002）的参数，并使用[Playground](https://platform.openai.com/playground)向其发送请求。

在作业完成后，模型应立即可用于推理使用。在某些情况下，您的模型可能需要几分钟才能准备好处理请求。如果对模型的请求超时或找不到模型名称，很可能是因为您的模型仍在加载中。如果发生这种情况，请稍等几分钟后再尝试。

<creatModelThree/>

您可以通过将模型名称传递给上述方式，以及我们的[GPT指南](../capabilityComponents/textGeneration.md#chat-completions-api)中所示的方式开始发送请求。


## 分析您的微调模型

我们提供了以下训练指标，这些指标是通过训练过程中计算得出的：训练损失（training loss）、训练标记准确率（training token accuracy）、测试损失（test loss）和测试标记准确率（test token accuracy）。这些统计数据旨在提供训练进展的合理检查（损失应该减少，标记准确率应该增加）。在运行一个活动的微调作业时，您可以查看一个包含一些有用指标的事件对象。

```json
{
    "object": "fine_tuning.job.event",
    "id": "ftevent-abc-123",
    "created_at": 1693582679,
    "level": "info",
    "message": "Step 100/100: training loss=0.00",
    "data": {
        "step": 100,
        "train_loss": 1.805623287509661e-5,
        "train_mean_token_accuracy": 1.0
    },
    "type": "metrics"
}
```

在微调作业完成后，您还可以通过[查询微调作业](../embeddings/embeddingsPage.md)、从result_files中提取文件ID，然后[检索该文件的内容](../embeddings/embeddingsPage.md)，来查看有关训练过程的指标。每个结果CSV文件包含以下列：step（步骤）、train_loss（训练损失）、train_accuracy（训练准确率）、valid_loss（验证损失）和valid_mean_token_accuracy（验证平均令牌准确率）。
```
step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy
1,1.52347,0.0,,
2,0.57719,0.0,,
3,3.63525,0.0,,
4,1.72257,0.0,,
5,1.52379,0.0,,
```
尽管指标可以提供一些帮助，但从微调模型生成样本能够提供最相关的模型质量感知。我们建议在测试集上从基础模型和微调模型生成样本，并将这些样本进行对比。测试集理想情况下应该包含您在生产环境中可能发送给模型的输入的全面分布。如果手动评估太费时，可以考虑使用我们的[Evals库](https://github.com/openai/evals)来自动化未来的评估工作。

### 数据质量的迭代

如果微调作业的结果不如您预期的好，请考虑以下调整训练数据集的方法：

- 收集针对剩余问题的示例
    - 如果模型在某些方面仍然表现不佳，请添加直接展示模型如何正确处理这些方面的训练示例。
- 仔细审查现有示例以查找问题
    - 如果您的模型存在语法、逻辑或风格问题，请检查您的数据是否存在相同的问题。例如，如果模型现在说“我将为您安排这次会议”（而实际上不应该这样说），请查看现有示例是否教导模型说它可以做它实际上无法做的新事物。
- 考虑数据的平衡性和多样性
    - 如果数据中60%的助手回复是“我无法回答这个问题”，但在推理时只有5%的回复应该是这样，您可能会得到过多的拒绝回答。
- 确保您的训练示例包含生成回复所需的所有信息。
    - 如果我们希望模型根据用户的个人特征进行赞美，而训练示例中包含的助手赞美的特征在之前的对话中找不到，模型可能会学会产生虚假信息。
- 查看训练示例中的一致性和一致性
    - 如果多个人创建了训练数据，模型的性能可能会受到人们之间一致性和一致性水平的限制。例如，在文本提取任务中，如果人们只同意提取片段的70%，那么模型很可能无法做得比这更好。
- 确保您的所有训练示例都以预期的格式进行推理。

### 数据量迭代

一旦您对示例的质量和分布感到满意，您可以考虑增加训练示例的数量。这往往有助于模型更好地学习任务，尤其是在可能的“边缘案例”上。我们预计每当您将训练示例数量翻倍时，都会有类似的改进程度。您可以通过以下方式粗略估计增加训练数据规模所带来的预期质量提升：
- 在您当前的数据集上进行微调
- 在您当前数据集的一半上进行微调
- 观察两者之间的质量差距
一般而言，如果您必须做出权衡，少量高质量的数据通常比大量低质量的数据更有效。

### 对超参数进行迭代优化

我们允许您指定以下超参数：
- epochs（训练轮数）
- 学习率乘数（learning rate multiplier）
- 批次大小（batch size）

我们建议最初在不指定任何这些超参数的情况下进行训练，让我们根据数据集大小为您选择默认值，然后在观察到以下情况时进行调整：
- 如果模型与预期不符合训练数据，可以通过增加1或2个epochs来解决
    - 这更常见于只有一个理想完成（或一小组类似的理想完成）的任务。一些例子包括分类、实体提取或结构化解析。这些通常是可以根据参考答案计算出最终准确度指标的任务。
- 如果模型的多样性不如预期，可以通过减少1或2个epochs来解决。
    - 这更常见于存在多种可能的良好完成的任务。
- 如果模型似乎没有收敛，可以增加学习率乘数来解决。

您可以按照下面的方式设置超参数：

<creatModelFour/>

## 微调示例

现在我们已经探索了微调API的基本知识，让我们来看看几个不同用例的微调生命周期。


<fineTuningExampleOne/>

## 从旧版模型迁移

对于从/v1/fine-tunes迁移到更新的/v1/fine_tuning/jobs API和更新的模型的用户，您可以预期的主要差异是更新的API。更新后的babbage-002和davinci-002型号保留了传统的即时完成对数据格式，以确保平稳过渡。新模型将支持4k代币上下文的微调，知识截止日期为2021年9月。


对于大多数任务，您应该期望gpt-3.5-turbo比gpt基本型号获得更好的性能。

## FAQ

### 什么时候应该使用微调与嵌入/检索增强生成？

带检索的嵌入最适合于需要具有相关上下文和信息的大型文档数据库的情况。

默认情况下，OpenAI的模型被训练成有用的多面手助手。微调可以用来制作一个关注范围很窄的模型，并表现出特定的根深蒂固的行为模式。检索策略可以用于在生成模型响应之前为其提供相关上下文，从而使新信息可用于模型。检索策略不是微调的替代方案，事实上可以是微调的补充。

您可以在我们的开发者日演讲中进一步探讨这些选项之间的差异：

this is a video about .............


### 是否可以微调 GPT-4或GPT-3.5-Turbo-16k？

GPT-4微调正在进行实验访问，符合条件的开发人员可以通过微调UI请求访问。目前，gpt-3.5-turbo-1106支持高达16K的上下文示例。


### 我如何知道我的微调模型是否真的比基本模型更好？

我们建议在聊天对话的测试集上从基本模型和微调模型生成样本，并并排比较样本。对于更全面的评估，可以考虑使用OpenAI评估框架来创建特定于您的用例的评估。

### 我可以继续微调已经微调过的模型吗？

是的，您可以在创建微调作业时将微调模型的名称传递到模型参数中。这将以微调模型为起点，开始新的微调工作。

### 如何估计微调模型的成本？

请参阅上面的[估算成本](../fineTuning/fineTuning.md#估计成本)部分。

### 新的微调端点是否仍然适用于跟踪度量的权重和偏差？

不，我们目前不支持这种集成，但正在努力在不久的将来实现它。

### 一次可以运行多少个微调工作？

有关限制的最新信息，请参阅我们的[速率限制指引](../embeddings/embeddingsPage.md)。

### 速率限制是如何在微调模型上工作的？

微调后的模型与它所基于的模型来自相同的共享速率限制。例如，如果您在给定时间段内使用标准gpt-3.5-turbo型号的一半TPM速率限制，则您从gpt-3.5-turbo微调的任何型号都只能访问TPM速率限制的剩余一半，因为容量在相同类型的所有型号之间共享。

换言之，从总吞吐量的角度来看，拥有经过微调的模型并不能让您有更多的能力使用我们的模型。


<script setup>
import checkDataFormat from './components/checkDataFormat.vue'
import uploadFile from './components/uploadFile.vue'
import creatModelOne from './components/creatModelOne.vue'
import creatModelTwo from './components/creatModelTwo.vue'
import creatModelThree from './components/creatModelThree.vue'
import creatModelFour from './components/creatModelFour.vue'
import fineTuningExampleOne from './components/fineTuningExampleOne.vue'
</script>