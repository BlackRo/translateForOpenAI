import {ref} from 'vue'

export default function(){


    const weatherExample_activeName = ref(["0"])
    const weatherExample_markdown_activeName = ref('Python')
    const weatherExample_python = '```python \n'
    +'from openai import OpenAI \n'
    +'import json \n'
    +'client = OpenAI() \n'
    +'# Example dummy function hard coded to return the same weather \n'
    +'# In production, this could be your backend API or an external API \n'
    +'def get_current_weather(location, unit="fahrenheit"): \n'
    +'  """Get the current weather in a given location""" \n'
    +'  if "tokyo" in location.lower(): \n'
    +'      return json.dumps({"location": "Tokyo", "temperature": "10", "unit": unit}) \n'
    +'  elif "san francisco" in location.lower(): \n'
    +'      return json.dumps({"location": "San Francisco", "temperature": "72", "unit": unit}) \n'
    +'  elif "paris" in location.lower(): \n'
    +'      return json.dumps({"location": "Paris", "temperature": "22", "unit": unit}) \n'
    +'  else: \n'
    +'      return json.dumps({"location": location, "temperature": "unknown"}) \n'
    +' \n'
    +'def run_conversation(): \n'
    +'  # Step 1: send the conversation and available functions to the model \n'
    +'  messages = [{"role": "user", "content": "What\'s the weather like in San Francisco, Tokyo, and Paris?"}] \n'
    +'  tools = [ \n'
    +'      { \n'
    +'          "type": "function", \n'
    +'          "function": { \n'
    +'              "name": "get_current_weather", \n'
    +'              "description": "Get the current weather in a given location", \n'
    +'              "parameters": { \n'
    +'                  "type": "object", \n'
    +'                  "properties": { \n'
    +'                      "location": { \n'
    +'                          "type": "string", \n'
    +'                          "description": "The city and state, e.g. San Francisco, CA", \n'
    +'                      }, \n'
    +'                      "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}, \n'
    +'                  }, \n'
    +'                  "required": ["location"], \n'
    +'              }, \n'
    +'          }, \n'
    +'      } \n'
    +'  ] \n'
    +'  response = client.chat.completions.create( \n'
    +'      model="gpt-3.5-turbo-1106", \n'
    +'      messages=messages, \n'
    +'      tools=tools, \n'
    +'      tool_choice="auto",  # auto is default, but we\'ll be explicit \n'
    +'  ) \n'
    +'  response_message = response.choices[0].message \n'
    +'  tool_calls = response_message.tool_calls \n'
    +'  # Step 2: check if the model wanted to call a function \n'
    +'  if tool_calls: \n'
    +'      # Step 3: call the function \n'
    +'      # Note: the JSON response may not always be valid; be sure to handle errors \n'
    +'      available_functions = { \n'
    +'          "get_current_weather": get_current_weather, \n'
    +'      }  # only one function in this example, but you can have multiple \n'
    +'      messages.append(response_message)  # extend conversation with assistant\'s reply \n'
    +'      # Step 4: send the info for each function call and function response to the model \n'
    +'      for tool_call in tool_calls: \n'
    +'          function_name = tool_call.function.name \n'
    +'          function_to_call = available_functions[function_name] \n'
    +'          function_args = json.loads(tool_call.function.arguments) \n'
    +'          function_response = function_to_call( \n'
    +'              location=function_args.get("location"), \n'
    +'              unit=function_args.get("unit"), \n'
    +'          ) \n'
    +'          messages.append( \n'
    +'              { \n'
    +'                  "tool_call_id": tool_call.id, \n'
    +'                  "role": "tool", \n'
    +'                  "name": function_name, \n'
    +'                  "content": function_response, \n'
    +'              } \n'
    +'          )  # extend conversation with function response \n'
    +'      second_response = client.chat.completions.create( \n'
    +'          model="gpt-3.5-turbo-1106", \n'
    +'          messages=messages, \n'
    +'      )  # get a new response from the model where it can see the function response \n'
    +'      return second_response \n'
    +'print(run_conversation()) \n'





    const weatherExample_node = '```js \n'
    +'import OpenAI from "openai" \n'
    +'const openai = new OpenAI(); \n'
    +'// Example dummy function hard coded to return the same weather \n'
    +'// In production, this could be your backend API or an external API \n'
    +'function getCurrentWeather(location, unit = "fahrenheit") { \n'
    +'  if (location.toLowerCase().includes("tokyo")) { \n'
    +'      return JSON.stringify({ location: "Tokyo", temperature: "10", unit: "celsius" }); \n'
    +'  } else if (location.toLowerCase().includes("san francisco")) { \n'
    +'      return JSON.stringify({ location: "San Francisco", temperature: "72", unit: "fahrenheit" }); \n'
    +'  } else if (location.toLowerCase().includes("paris")) { \n'
    +'      return JSON.stringify({ location: "Paris", temperature: "22", unit: "fahrenheit" }); \n'
    +'  } else { \n'
    +'      return JSON.stringify({ location, temperature: "unknown" }); \n'
    +'  } \n'
    +'} \n'
    +' \n'
    +'async function runConversation() { \n'
    +'  // Step 1: send the conversation and available functions to the model \n'
    +'  const messages = [ \n'
    +'      { role: "user", content: "What\'s the weather like in San Francisco, Tokyo, and Paris?" }, \n'
    +'  ]; \n'
    +'  const tools = [ \n'
    +'      { \n'
    +'          type: "function", \n'
    +'          function: { \n'
    +'              name: "get_current_weather", \n'
    +'              description: "Get the current weather in a given location", \n'
    +'              parameters: { \n'
    +'                  type: "object", \n'
    +'                  properties: { \n'
    +'                      location: { \n'
    +'                          type: "string", \n'
    +'                          description: "The city and state, e.g. San Francisco, CA", \n'
    +'                      }, \n'
    +'                      unit: { type: "string", enum: ["celsius", "fahrenheit"] }, \n'
    +'                  }, \n'
    +'                  required: ["location"], \n'
    +'              }, \n'
    +'          }, \n'
    +'      }, \n'
    +'  ]; \n'
    +' \n'
    +'  const response = await openai.chat.completions.create({ \n'
    +'      model: "gpt-3.5-turbo-1106", \n'
    +'      messages: messages, \n'
    +'      tools: tools, \n'
    +'      tool_choice: "auto", // auto is default, but we\'ll be explicit \n'
    +'  }); \n'
    +'  const responseMessage = response.choices[0].message; \n'
    +'  // Step 2: check if the model wanted to call a function \n'
    +'  const toolCalls = responseMessage.tool_calls; \n'
    +'  if (responseMessage.tool_calls) { \n'
    +'      // Step 3: call the function \n'
    +'      // Note: the JSON response may not always be valid; be sure to handle errors \n'
    +'      const availableFunctions = { \n'
    +'          get_current_weather: getCurrentWeather, \n'
    +'      }; // only one function in this example, but you can have multiple \n'
    +'      messages.push(responseMessage); // extend conversation with assistant\'s reply \n'
    +'      for (const toolCall of toolCalls) { \n'
    +'          const functionName = toolCall.function.name; \n'
    +'          const functionToCall = availableFunctions[functionName]; \n'
    +'          const functionArgs = JSON.parse(toolCall.function.arguments); \n'
    +'          const functionResponse = functionToCall( \n'
    +'              functionArgs.location, \n'
    +'              functionArgs.unit \n'
    +'          ); \n'
    +'          messages.push({ \n'
    +'              tool_call_id: toolCall.id, \n'
    +'              role: "tool", \n'
    +'              name: functionName, \n'
    +'              content: functionResponse, \n'
    +'          }); // extend conversation with function response \n'
    +'      } \n'
    +'      const secondResponse = await openai.chat.completions.create({ \n'
    +'          model: "gpt-3.5-turbo-1106", \n'
    +'          messages: messages, \n'
    +'      }); // get a new response from the model where it can see the function response \n'
    +'      return secondResponse.choices; \n'
    +'  } \n'
    +'} \n'
    +'runConversation().then(console.log).catch(console.error);\n'

    return {
        weatherExample_markdown_activeName,
        weatherExample_python,
        weatherExample_node,
        weatherExample_activeName
    }

}